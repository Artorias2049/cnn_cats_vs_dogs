{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and pre-processing the images\n",
    "DATADIR = \"Datasets/PetImages\"\n",
    "CATEGORIES = [\"Cat\", \"Dog\"]\n",
    "\n",
    "training_data = []\n",
    "IMG_SIZE = 70\n",
    "\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR, category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img))#, cv2.IMREAD_GRAYSCALE)\n",
    "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                training_data.append([new_array, class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomizing the data\n",
    "import random\n",
    "\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "y = np.array(y).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255.0 #Converting the numbers to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X[: 1000]   #Validation set\n",
    "y_val = y[: 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = int(X.shape[0]*0.2) #Deciding what % of data will be in the test set. Currently set to 20%\n",
    "X_train, X_test = X[index:], X[:index]\n",
    "y_train, y_test = y[index:], y[:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying out different combinations of layers\n",
    "dense_layers = [1,2]\n",
    "layer_sizes = [64, 128]\n",
    "conv_layers = [1,2,3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME = f'dense{dense_layer}-layer_size{layer_size}-conv_layer{conv_layer}-{int(time.time())}'\n",
    "            tensorboard = TensorBoard(log_dir=f'logs/{NAME}')\n",
    "            \n",
    "            filepath=f\"checkpoints/weights-improvement-d{dense_layer}-l{layer_size}-c{conv_layer}.hdf5\"\n",
    "            checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "            \n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "            \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3,3), activation=\"relu\", input_shape = X_train.shape[1:]))\n",
    "            model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "            for l in range(conv_layer - 1):\n",
    "                model.add(Conv2D(layer_size, (3,3), activation=\"relu\"))\n",
    "                model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "            model.add(Flatten())\n",
    "\n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size, activation= \"relu\"))\n",
    "                model.add(Dropout(0.2))\n",
    "\n",
    "            model.add(Dense(1, activation= \"sigmoid\"))\n",
    "\n",
    "            model.compile(loss=\"binary_crossentropy\", \n",
    "                            optimizer=\"adam\", \n",
    "                            metrics=[\"accuracy\"])\n",
    "\n",
    "            model.fit(X_train, y_train, batch_size=32, \n",
    "                            epochs=25, \n",
    "                            validation_data=(X_test, y_test),\n",
    "                            callbacks= [tensorboard, checkpoint, early_stop])\n",
    "            \n",
    "            model.load_weights(filepath)\n",
    "            model.save(f'saved_models/cats_dogs_classifier-d{dense_layer}-l{layer_size}-c{conv_layer}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best settings for good results\n",
    "dense_layer = 1\n",
    "layer_size = 64\n",
    "conv_layer = 3\n",
    "\n",
    "NAME = f'dense{dense_layer}-layer_size{layer_size}-conv_layer{conv_layer}-{int(time.time())}'\n",
    "tensorboard = TensorBoard(log_dir=f'logs/{NAME}')\n",
    "            \n",
    "filepath=f\"checkpoints/weights-improvement-d{dense_layer}-l{layer_size}-c{conv_layer}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "            \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "            \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(layer_size, (3,3), activation=\"relu\", input_shape = X_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "model.add(Conv2D(layer_size, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "model.add(Conv2D(layer_size, (3,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(layer_size, activation= \"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation= \"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", \n",
    "                optimizer=\"adam\", \n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, \n",
    "                epochs=25, \n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks= [tensorboard, checkpoint, early_stop])\n",
    "\n",
    "model.load_weights(filepath)\n",
    "model.save(f'saved_models/cats_dogs_classifier-d{dense_layer}-l{layer_size}-c{conv_layer}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('checkpoints/weights-improvement-d1-l64-c3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_val)\n",
    "y_predicted= (y_predicted>0.5)\n",
    "\n",
    "#Tests to check if used model is good\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "print(f'Accuracy score is {accuracy_score(y_val, y_predicted)}')\n",
    "print(f'\\nClassification report:\\n{classification_report(y_val, y_predicted)}')\n",
    "print(f'\\nConfusion matrix:\\n{confusion_matrix(y_predicted, y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making custom predictions\n",
    "def make_prediction(test_img):\n",
    "    t_img = cv2.imread(test_img)#, cv2.IMREAD_GRAYSCALE)\n",
    "    t_img_rz = cv2.resize(t_img, (IMG_SIZE, IMG_SIZE))\n",
    "    t_img_rs = t_img_rz.reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "    result = model.predict(t_img_rs)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = make_prediction('smile_cat.jpg')\n",
    "print(CATEGORIES[int(pred[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90b95b77899b776058dadf85fce44ea025abfdbce1476a955d85993506b09ab7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
